{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b40532e",
   "metadata": {},
   "source": [
    "# CYBER Master\n",
    "\n",
    "This project is to process the data of recorded voices and baseline measurements for the attachment style interaction with depression and anxiety, in a sense this will work a the ETL or pre-processing of the data.\n",
    "\n",
    "This notebook has a single purpose expressed in the following steps:\n",
    "- Gather the answers from the survey DB\n",
    "- Process the answers by uniques and present a report of the demographics, attachment styles and HADS answers\n",
    "- From each answer, get the recordings from blob storage.\n",
    "- For each recording transcribe them and associate them to the answer\n",
    "- Store the resulting data in the DB for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77a6d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Import the necessary libraries\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import subprocess\n",
    "import requests\n",
    "from typing import List\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "CONSENT_SURVEY_ID = \"8\"\n",
    "STUDY_SURVEY_ID = \"7\"\n",
    "OLLAMA_VERSION = \"v0.6\"\n",
    "\n",
    "QUESTIONS = {\n",
    "    \"en\": {\n",
    "        \"1\": \"What’s happening in the picture?\",\n",
    "        \"2\": \"What led up to that scene?\",\n",
    "        \"3\": \"What the characters are thinking or feeling?\",\n",
    "        \"4\": \"What might happen next?\",\n",
    "    },\n",
    "    \"es\": {\n",
    "        \"1\": \"¿Qué está pasando en la imagen?\",\n",
    "        \"2\": \"¿Qué eventos llevaron a esta escena?\",\n",
    "        \"3\": \"¿Qué están sintiendo o pensando los personajes?\",\n",
    "        \"4\": \"¿Qué ocurrirá a continuación?\",\n",
    "    },\n",
    "}\n",
    "\n",
    "IMAGE_KEYS = [\n",
    "    \"Image 1 - 1\",\n",
    "    \"Image 1 - 2\",\n",
    "    \"Image 1 - 3\",\n",
    "    \"Image 1 - 4\",\n",
    "    \"Image 2 - 1\",\n",
    "    \"Image 2 - 2\",\n",
    "    \"Image 2 - 3\",\n",
    "    \"Image 2 - 4\",\n",
    "    \"Image 3 - 1\",\n",
    "    \"Image 3 - 2\",\n",
    "    \"Image 3 - 3\",\n",
    "    \"Image 3 - 4\",\n",
    "    \"Image 4 - 1\",\n",
    "    \"Image 4 - 2\",\n",
    "    \"Image 4 - 3\",\n",
    "    \"Image 4 - 4\",\n",
    "    \"Image 5 -1\",\n",
    "    \"Image 5 - 2\",\n",
    "    \"Image 5 - 3\",\n",
    "    \"Image 5 - 4\",\n",
    "    \"Image 6 - 1\",\n",
    "    \"Image 6 - 2\",\n",
    "    \"Image 6 - 3\",\n",
    "    \"Image 6 - 4\",\n",
    "    \"Image 7 - 1\",\n",
    "    \"Image 7 - 2\",\n",
    "    \"Image 7 - 3\",\n",
    "    \"Image 7 - 4\",\n",
    "    \"Image 8 - 1\",\n",
    "    \"Image 8 - 2\",\n",
    "    \"Image 8 - 3\",\n",
    "    \"Image 8 - 4\",\n",
    "]\n",
    "\n",
    "\n",
    "def transcribe_audio(file_path, language=\"en\"):\n",
    "    # Use the local vosk via CLI to get the transcription\n",
    "    result_file = file_path.replace(\".wav\", \".txt\")\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"vosk-transcriber\",\n",
    "            \"-i\",\n",
    "            file_path,\n",
    "            \"-l\",\n",
    "            language,\n",
    "            \"-o\",\n",
    "            result_file,\n",
    "            \"--log-level\",\n",
    "            \"INFO\",\n",
    "        ]\n",
    "    )\n",
    "    # Read the transcription\n",
    "    with open(result_file, \"r\") as transcription_file:\n",
    "        transcription = transcription_file.read()\n",
    "        return transcription\n",
    "\n",
    "\n",
    "def request_llm_analysis(prompts: List[str], language=\"en\"):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    prompt = (f\"Respond with the attachment style, a percentage for the likelihood of developing anxiety and a percentage of developing depression from the following answers,\"\n",
    "              + \" give just one conscise answer with no explanations. Consider only the following answers:\\n\")\n",
    "    prompt += \"\\n\".join(prompts)\n",
    "    data = {\n",
    "        \"prompt\": prompt,\n",
    "        \"model\": OLLAMA_VERSION,\n",
    "    }\n",
    "    response = requests.post(url, json=data)\n",
    "    result = \"\"\n",
    "    for line in response.text.split(\"\\n\"):\n",
    "        if '\"done\": true' in line or not line:\n",
    "            return result\n",
    "        else:\n",
    "            try:\n",
    "                result += json.loads(line)[\"response\"]\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(line)\n",
    "                continue\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def score_HADS(answers: dict) -> tuple:\n",
    "    anxiety_items = {\n",
    "        \"HADS 1\": {\"0\": 0, \"1\": 1, \"2\": 2, \"3\": 3},\n",
    "        \"HADS 5\": {\"0\": 0, \"1\": 1, \"2\": 2, \"3\": 3},\n",
    "        \"HADS 6\": {\"0\": 0, \"1\": 1, \"2\": 2, \"3\": 3},\n",
    "        \"HADS 8\": {\"0\": 0, \"1\": 1, \"2\": 2, \"3\": 3},\n",
    "        \"HADS 9\": {\"0\": 0, \"1\": 1, \"2\": 2, \"3\": 3},\n",
    "        \"HADS 12\": {\"0\": 0, \"1\": 1, \"2\": 2, \"3\": 3},\n",
    "        \"HADS 13\": {\"0\": 3, \"1\": 2, \"2\": 1, \"3\": 0},\n",
    "    }\n",
    "    depression_items = {\n",
    "        \"HADS 2\": {\"0\": 0, \"1\": 1, \"2\": 2, \"3\": 3},\n",
    "        \"HADS 3\": {\"0\": 0, \"1\": 1, \"2\": 2, \"3\": 3},\n",
    "        \"HADS 4\": {\"0\": 3, \"1\": 2, \"2\": 1, \"3\": 0},\n",
    "        \"HADS 7\": {\"0\": 3, \"1\": 2, \"2\": 1, \"3\": 0},\n",
    "        \"HADS 10\": {\"0\": 3, \"1\": 2, \"2\": 1, \"3\": 0},\n",
    "        \"HADS 11\": {\"0\": 3, \"1\": 2, \"2\": 1, \"3\": 0},\n",
    "        \"HADS 14\": {\"0\": 0, \"1\": 1, \"2\": 2, \"3\": 3},\n",
    "    }\n",
    "\n",
    "    anxiety_score = 0\n",
    "    depression_score = 0\n",
    "\n",
    "    for item, answer in answers.items():\n",
    "        if item in anxiety_items:\n",
    "            anxiety_score += anxiety_items[item][answer]\n",
    "        elif item in depression_items:\n",
    "            depression_score += depression_items[item][answer]\n",
    "\n",
    "    return anxiety_score, depression_score\n",
    "\n",
    "\n",
    "def score_RQ(answers: dict) -> dict:\n",
    "    anxiety_score = 0\n",
    "    avoidance_score = 0\n",
    "\n",
    "    styles = {\n",
    "        \"A\": \"Secure\",\n",
    "        \"B\": \"Fearful\",\n",
    "        \"C\": \"Preoccupied\",\n",
    "        \"D\": \"Dismissing\",\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    for item, answer in answers.items():\n",
    "        if item == \"RQ1\":\n",
    "            results[\"style\"] = styles[answer]\n",
    "        elif item == \"RQ2\":\n",
    "            anxiety_score += int(answer)\n",
    "            avoidance_score += int(answer)\n",
    "        elif item == \"RQ3\":\n",
    "            anxiety_score -= int(answer)\n",
    "            avoidance_score -= int(answer)\n",
    "        elif item == \"RQ4\":\n",
    "            anxiety_score -= int(answer)\n",
    "            avoidance_score += int(answer)\n",
    "        elif item == \"RQ5\":\n",
    "            anxiety_score += int(answer)\n",
    "            avoidance_score -= int(answer)\n",
    "\n",
    "    results[\"anxiety\"] = anxiety_score\n",
    "    results[\"avoidance\"] = avoidance_score\n",
    "    return results\n",
    "\n",
    "def get_only_numbers(text: str) -> str:\n",
    "    return ''.join(filter(str.isdigit, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35256663",
   "metadata": {},
   "source": [
    "## Connect to the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3dbb8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "connection = mysql.connector.connect(\n",
    "    user=os.getenv(\"DB_USERNAME\"),\n",
    "    password=os.getenv(\"DB_PASSWORD\"),\n",
    "    host=os.getenv(\"DB_HOST\"),\n",
    "    database=os.getenv(\"DB_NAME\"),\n",
    "    port=os.getenv(\"DB_PORT\"),\n",
    ")\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# query the answers table\n",
    "query = (\n",
    "    f\"SELECT * FROM surveys_answer where surveys_answer.survey_id = {STUDY_SURVEY_ID}\"\n",
    ")\n",
    "cursor.execute(query)\n",
    "answers = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2941470e-b36c-4949-a870-3b8beeefc03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en': 0, 'es': 10, 'pt': 0}\n"
     ]
    }
   ],
   "source": [
    "# Parse the answers\n",
    "json_answers = []\n",
    "\n",
    "for answer in answers:\n",
    "    dict_answer = json.loads(json.loads(answer[1]))\n",
    "    if dict_answer.get(\"response-uuid\") not in [\n",
    "        answer.get(\"response-uuid\") for answer in json_answers\n",
    "    ]:\n",
    "        json_answers.append({\"id\": answer[0], **json.loads(json.loads(answer[1]))})\n",
    "    else:\n",
    "        for json_answer in json_answers:\n",
    "            if json_answer.get(\"response-uuid\") == dict_answer.get(\"response-uuid\"):\n",
    "                json_answer.update(json.loads(json.loads(answer[1])))\n",
    "                break\n",
    "full_answers = [answer for answer in json_answers if len(answer.keys()) == 62]\n",
    "# get answers by language in a dictionary\n",
    "answers_by_language = {}\n",
    "answers_by_language = {\n",
    "    \"en\": len([answer for answer in full_answers if answer[\"language\"] == \"en\"]),\n",
    "    \"es\": len([answer for answer in full_answers if answer[\"language\"] == \"es\"]),\n",
    "    \"pt\": len([answer for answer in full_answers if answer[\"language\"] == \"pt\"]),\n",
    "}\n",
    "print(answers_by_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f75de8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131, 'recordings/blob', None, 'en', '88bd9dcedc08411a800024e83806387c')\n"
     ]
    }
   ],
   "source": [
    "# Get the recording info from the recording table\n",
    "recordings_query = f\"SELECT * FROM surveys_recording\"  # where surveys_recording.answer_id in ({','.join([str(answer.get('id')) for answer in full_answers])})\"\n",
    "cursor.execute(recordings_query)\n",
    "recordings = cursor.fetchall()\n",
    "print(recordings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68bd095a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 573, 'language': 'es', 'response-uuid': 'ab7de4d7-3475-4c59-b345-f35312272095', 'Demographic - Age': '57', 'Demographic - Gender': 'female', 'Demographic - Education level': 'bachelor', 'Demographic - Relationship status': 'married', 'Demographic - Native speaker': 'yes', 'Demographic - speech diagnosis': 'no', 'Demographic - psychiatric disorder': 'no', 'Demographic - Substances': 'no', 'RQ1': 'A', 'RQ2': '6', 'RQ3': '1', 'RQ4': '2', 'RQ5': '2', 'HADS 1': '1', 'HADS 2': '0', 'HADS 3': '0', 'HADS 4': '3', 'HADS 5': '0', 'HADS 6': '0', 'HADS 7': '3', 'HADS 8': '0', 'HADS 9': '1', 'HADS 10': '3', 'HADS 11': '2', 'HADS 12': '0', 'HADS 13': '3', 'HADS 14': '3', 'Image 1 - 1': 'blob_IOMVHa8.wav', 'Image 1 - 2': 'blob_sECyzPP.wav', 'Image 1 - 3': 'blob_slb6iz5.wav', 'Image 1 - 4': 'blob_DQxVBcd.wav', 'Image 2 - 1': 'blob_2Vmp8iJ.wav', 'Image 2 - 2': 'blob_LPCZEan.wav', 'Image 2 - 3': 'blob_Ig0pubk.wav', 'Image 2 - 4': 'blob_3qVhU86.wav', 'Image 3 - 1': 'blob_jxf0b6n.wav', 'Image 3 - 2': 'blob_YoO5hBg.wav', 'Image 3 - 3': 'blob_UuLDNVa.wav', 'Image 3 - 4': 'blob_yZDvWm6.wav', 'Image 4 - 1': 'blob_1bf8seO.wav', 'Image 4 - 2': 'blob_AizgeKY.wav', 'Image 4 - 3': 'blob_XiFfrS5.wav', 'Image 4 - 4': 'blob_gJhR2F7.wav', 'Image 5 -1': 'blob_R4fDKpl.wav', 'Image 5 - 2': 'blob_xLrJUVr.wav', 'Image 5 - 3': 'blob_SnzkWqe.wav', 'Image 5 - 4': 'blob_LMW0D8I.wav', 'Image 6 - 1': 'blob_F3QyS4e.wav', 'Image 6 - 2': 'blob_U446ase.wav', 'Image 6 - 3': 'blob_u4ORCDN.wav', 'Image 6 - 4': 'blob_5Vgn0g9.wav', 'Image 7 - 1': 'blob_geIxP37.wav', 'Image 7 - 2': 'blob_zlUV0uS.wav', 'Image 7 - 3': 'blob_oesOVcm.wav', 'Image 7 - 4': 'blob_2g7kPhg.wav', 'Image 8 - 1': 'blob_1YOcXNS.wav', 'Image 8 - 2': 'blob_XddP7gr.wav', 'Image 8 - 3': 'blob_QcR0rYv.wav', 'Image 8 - 4': '[object Blob]'}\n"
     ]
    }
   ],
   "source": [
    "for answer in full_answers:\n",
    "    for recording in recordings:\n",
    "        for key, value in answer.items():\n",
    "            if not isinstance(value, str) or len(value) < 4:\n",
    "                continue\n",
    "            if value.replace(\"-\", \"\") != recording[-1]:\n",
    "                continue\n",
    "            answer[key] = f\"{recording[1].replace('recordings/', '')}.wav\"\n",
    "\n",
    "\n",
    "print(full_answers[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26746968",
   "metadata": {},
   "source": [
    "## Getting the recordings per answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32405ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "CONTAINER_NAME = \"recordings\"\n",
    "\n",
    "blob_service = BlobServiceClient.from_connection_string(\n",
    "    os.getenv(\"AZURE_CONNECTION_STRING\")\n",
    ")\n",
    "local_path = \"./recordings\"\n",
    "# create the folder if not alrady\n",
    "if not os.path.exists(local_path):\n",
    "    os.mkdir(local_path)\n",
    "\n",
    "# Sanity check, listing the blobs in the container\n",
    "container_client = blob_service.get_container_client(CONTAINER_NAME)\n",
    "blob_list = []\n",
    "for blob in container_client.list_blobs():\n",
    "    if \"blob\" in blob.name:\n",
    "        blob_list.append(blob.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e82b756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "689\n"
     ]
    }
   ],
   "source": [
    "# Download the blobs into the recordings folder if not already downloaded\n",
    "for blob in blob_list:\n",
    "    result_file_name = blob.split(\"/\")[-1]\n",
    "    result_file_name = f\"{result_file_name}.wav\"\n",
    "    if result_file_name not in os.listdir(local_path):\n",
    "        with open(file=f\"{local_path}/{result_file_name}\", mode=\"wb\") as audio_file:\n",
    "            download_stream = container_client.download_blob(blob)\n",
    "            audio_file.write(download_stream.readall())\n",
    "\n",
    "# list all the files in the folder\n",
    "files = os.listdir(local_path)\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ed8fae",
   "metadata": {},
   "source": [
    "## Vosk\n",
    "\n",
    "We are using [vosk](https://alphacephei.com/vosk/) to transcribe the audio into text, then assign it to the answer for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d1ab00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File blob_nrnej2h.wav already transcribed\n",
      "File blob_6Pu57EX.wav already transcribed\n",
      "File blob_DSHsnaO.wav already transcribed\n",
      "File blob_tLTvyIY.wav already transcribed\n",
      "File blob_oQmoc6K.wav already transcribed\n",
      "File blob_wVigQOC.wav already transcribed\n",
      "File blob_81DcWTq.wav already transcribed\n",
      "File blob_yiLAcqi.wav already transcribed\n",
      "File blob_2ZzcIQ2.wav already transcribed\n",
      "File blob_ijGVE3a.wav already transcribed\n",
      "File blob_9nF4u9E.wav already transcribed\n",
      "File blob_Kemd4qU.wav already transcribed\n",
      "File blob_rbSy8qO.wav already transcribed\n",
      "File blob_7J4aQii.wav already transcribed\n",
      "File blob_PmIsMAa.wav already transcribed\n",
      "File blob_hh9HFZm.wav already transcribed\n",
      "File blob_7oAe8QB.wav already transcribed\n",
      "File blob_V6S1D46.wav already transcribed\n",
      "File blob_Pbfp6Ij.wav already transcribed\n",
      "File blob_ttmhOBh.wav already transcribed\n",
      "File blob_fpWJDLo.wav already transcribed\n",
      "File blob_hmRuNX3.wav already transcribed\n",
      "File blob_KRBmVTH.wav already transcribed\n",
      "File blob_7tnXxx6.wav already transcribed\n",
      "File blob_8xq8Zmj.wav already transcribed\n",
      "File blob_8ailPdr.wav already transcribed\n",
      "File blob_XP31N65.wav already transcribed\n",
      "File blob_LYh53xa.wav already transcribed\n",
      "File blob_5kV4sL1.wav already transcribed\n",
      "File blob_xCyVp6C.wav already transcribed\n",
      "File blob_dMwnSGo.wav already transcribed\n",
      "File blob_WtJk6Zs.wav already transcribed\n",
      "File blob_xoAsSkB.wav already transcribed\n",
      "File blob_Un4ZCyy.wav already transcribed\n",
      "File blob_Fnsk9uW.wav already transcribed\n",
      "File blob_m8VvwVp.wav already transcribed\n",
      "File blob_eAGyFpt.wav already transcribed\n",
      "File blob_bgRSB6S.wav already transcribed\n",
      "File blob_TcV426a.wav already transcribed\n",
      "File blob_CwRAm6F.wav already transcribed\n",
      "File blob_ysMlbNz.wav already transcribed\n",
      "File blob_VHusJ24.wav already transcribed\n",
      "File blob_l8vHTCH.wav already transcribed\n",
      "File blob_mzHV8h4.wav already transcribed\n",
      "File blob_2VtTAWp.wav already transcribed\n",
      "File blob_2gpBWca.wav already transcribed\n",
      "File blob_hWLLj1S.wav already transcribed\n",
      "File blob_QW0aWvT.wav already transcribed\n",
      "File blob_zIk60DR.wav already transcribed\n",
      "File blob_BronUc5.wav already transcribed\n",
      "File blob_xLJQSxo.wav already transcribed\n",
      "File blob_qpsnoK6.wav already transcribed\n",
      "File blob_EXzb1bQ.wav already transcribed\n",
      "File blob_o3Msoof.wav already transcribed\n",
      "File blob_95TeMOE.wav already transcribed\n",
      "File blob_lLG2I1E.wav already transcribed\n",
      "File blob_NSREms1.wav already transcribed\n",
      "File blob_TljoaDG.wav already transcribed\n",
      "File blob_NATpO7j.wav already transcribed\n",
      "File blob_WCVMaKV.wav already transcribed\n",
      "File blob_K2aRULv.wav already transcribed\n",
      "File blob_PTCgMjY.wav already transcribed\n",
      "File blob_hMqpLyK.wav already transcribed\n",
      "File blob_w3szq6A.wav already transcribed\n",
      "File blob_S4pUIs2.wav already transcribed\n",
      "File blob_Uj8tTnA.wav already transcribed\n",
      "File blob_NwDgzVr.wav already transcribed\n",
      "File blob_SRj4DZu.wav already transcribed\n",
      "File blob_xsN9BW2.wav already transcribed\n",
      "File blob_mCmMPIs.wav already transcribed\n",
      "File blob_EUbTPTB.wav already transcribed\n",
      "File blob_bAB8zlF.wav already transcribed\n",
      "File blob_VI1I5zh.wav already transcribed\n",
      "File blob_RSFuEWK.wav already transcribed\n",
      "File blob_nqVXERK.wav already transcribed\n",
      "File blob_KfgRZhr.wav already transcribed\n",
      "File blob_dePXZjz.wav already transcribed\n",
      "File blob_nCmclwt.wav already transcribed\n",
      "File blob_mgbbEdA.wav already transcribed\n",
      "File blob_eLXcJ3y.wav already transcribed\n",
      "File blob_pywSCmy.wav already transcribed\n",
      "File blob_1hH0rRn.wav already transcribed\n",
      "File blob_YyoDjaN.wav already transcribed\n",
      "File blob_BPT1Gu8.wav already transcribed\n",
      "File blob_pI85x7S.wav already transcribed\n",
      "File blob_AjoGJAq.wav already transcribed\n",
      "File blob_BcBEFuf.wav already transcribed\n",
      "File blob_irCoUED.wav already transcribed\n",
      "File blob_9sLOeIH.wav already transcribed\n",
      "File blob_fr8YGax.wav already transcribed\n",
      "File blob_ankVLQI.wav already transcribed\n",
      "File blob_50EEhUC.wav already transcribed\n",
      "File blob_B1bfVo1.wav already transcribed\n",
      "File blob_6JcKY9o.wav already transcribed\n",
      "File blob_xSIG2aR.wav already transcribed\n",
      "File blob_DgcpRXc.wav already transcribed\n",
      "File blob_wIrxRfz.wav already transcribed\n",
      "File blob_aYgMzQG.wav already transcribed\n",
      "File blob_HGppiM4.wav already transcribed\n",
      "File blob_Cem0Dep.wav already transcribed\n",
      "File blob_5xG0jNm.wav already transcribed\n",
      "File blob_0pKgouf.wav already transcribed\n",
      "File blob_ZH9lv7S.wav already transcribed\n",
      "File blob_Ciesglx.wav already transcribed\n",
      "File blob_8A9Slb3.wav already transcribed\n",
      "File blob_QEYgCLO.wav already transcribed\n",
      "File blob_v8YCzXb.wav already transcribed\n",
      "File blob_VklwsJu.wav already transcribed\n",
      "File blob_wTwVNF3.wav already transcribed\n",
      "File blob_T6aJlV5.wav already transcribed\n",
      "File blob_5AEFsq2.wav already transcribed\n",
      "File blob_rS2qbF8.wav already transcribed\n",
      "File blob_CyMmrdT.wav already transcribed\n",
      "File blob_7VDMwcG.wav already transcribed\n",
      "File blob_7dSqZVM.wav already transcribed\n",
      "File blob_kPUQOTY.wav already transcribed\n",
      "File blob_difoBaE.wav already transcribed\n",
      "File blob_UC2P4ll.wav already transcribed\n",
      "File blob_cwQHx5K.wav already transcribed\n",
      "File blob_oDp0vBJ.wav already transcribed\n",
      "File blob_KkXBQCy.wav already transcribed\n",
      "File blob_J8iOTk0.wav already transcribed\n",
      "File blob_AfJEap9.wav already transcribed\n",
      "File blob_BBkRF4d.wav already transcribed\n",
      "File blob_A1u0P6R.wav already transcribed\n",
      "File blob_TI9tAK9.wav already transcribed\n",
      "File blob_VLriBAm.wav already transcribed\n",
      "File blob_AWdlUMd.wav already transcribed\n",
      "File blob_lmlCmUO.wav already transcribed\n",
      "File blob_YGAO2dr.wav already transcribed\n",
      "File blob_64EeeEr.wav already transcribed\n",
      "File blob_QV38tLo.wav already transcribed\n",
      "File blob_6q5UD9u.wav already transcribed\n",
      "File blob_OeHKbUn.wav already transcribed\n",
      "File blob_Qj6arYE.wav already transcribed\n",
      "File blob_XSAZhq5.wav already transcribed\n",
      "File blob_CADd8UF.wav already transcribed\n",
      "File blob_9LPc0eK.wav already transcribed\n",
      "File blob_SvDV2IY.wav already transcribed\n",
      "File blob_UMreAzD.wav already transcribed\n",
      "File blob_8xRgBXL.wav already transcribed\n",
      "File blob_q68ECXG.wav already transcribed\n",
      "File blob_8x3cqgj.wav already transcribed\n",
      "File blob_aOW0Rtj.wav already transcribed\n",
      "File blob_qespNlR.wav already transcribed\n",
      "File blob_HYGR6iy.wav already transcribed\n",
      "File blob_pE8ZlqD.wav already transcribed\n",
      "File blob_LiPA0B6.wav already transcribed\n",
      "File blob_Lx3v5fB.wav already transcribed\n",
      "File blob_TWMVUQB.wav already transcribed\n",
      "File blob_mC8tPZl.wav already transcribed\n",
      "File blob_GgOtzFr.wav already transcribed\n",
      "File blob_pC4omcM.wav already transcribed\n",
      "File blob_Yryfacw.wav already transcribed\n",
      "File blob_cop9uen.wav already transcribed\n",
      "File blob_0dg67f7.wav already transcribed\n",
      "File blob_4FNbYL5.wav already transcribed\n",
      "File blob_ExPw7mp.wav already transcribed\n",
      "File blob_r95qRCb.wav already transcribed\n",
      "File blob_m9yUrnY.wav already transcribed\n",
      "File blob_xT63RN7.wav already transcribed\n",
      "File blob_YWuDrLu.wav already transcribed\n",
      "File blob_pfY8kof.wav already transcribed\n",
      "File blob_2RPp7IL.wav already transcribed\n",
      "File blob_s2mq9GQ.wav already transcribed\n",
      "File blob_6ZacjsO.wav already transcribed\n",
      "File blob_mB0qC1D.wav already transcribed\n",
      "File blob_Ebg1v4V.wav already transcribed\n",
      "File blob_YKzkyd5.wav already transcribed\n",
      "File blob_ZFPmXEq.wav already transcribed\n",
      "File blob_LkbnZCC.wav already transcribed\n",
      "File blob_LpFr4PL.wav already transcribed\n",
      "File blob_dcLKbyn.wav already transcribed\n",
      "File blob_71SqiZL.wav already transcribed\n",
      "File blob_idzufeR.wav already transcribed\n",
      "File blob_M0sEwF0.wav already transcribed\n",
      "File blob_3yaWCGJ.wav already transcribed\n",
      "File blob_P7Us8VH.wav already transcribed\n",
      "File blob_Gpjjc6z.wav already transcribed\n",
      "File blob_UFJ8oTD.wav already transcribed\n",
      "File blob_YQFtegp.wav already transcribed\n",
      "File blob_Kk1n3il.wav already transcribed\n",
      "File blob_3b58uCa.wav already transcribed\n",
      "File blob_mkepURZ.wav already transcribed\n",
      "File blob_P5NaGVd.wav already transcribed\n",
      "File blob_sqD7oxu.wav already transcribed\n",
      "File blob_S1ljUnT.wav already transcribed\n",
      "File blob_lsGCqey.wav already transcribed\n",
      "File blob_q2uxPBR.wav already transcribed\n",
      "File blob_1HKVjAo.wav already transcribed\n",
      "File blob_k91Seg4.wav already transcribed\n",
      "File blob_deGyzEL.wav already transcribed\n",
      "File blob_0U8QPi2.wav already transcribed\n",
      "File blob_jhZQpyI.wav already transcribed\n",
      "File blob_dILuybz.wav already transcribed\n",
      "File blob_vpUE0yK.wav already transcribed\n",
      "File blob_Ezqujyx.wav already transcribed\n",
      "File blob_bmR1kJY.wav already transcribed\n",
      "File blob_OrIPjvT.wav already transcribed\n",
      "File blob_MplFhWF.wav already transcribed\n",
      "File blob_aLE1XNk.wav already transcribed\n",
      "File blob_At6o77j.wav already transcribed\n",
      "File blob_KwNA3K3.wav already transcribed\n",
      "File blob_BtuJiLL.wav already transcribed\n",
      "File blob_PDtcIvb.wav already transcribed\n",
      "File blob_EUmPOfF.wav already transcribed\n",
      "File blob_BfBlUvz.wav already transcribed\n",
      "File blob_H5DHD1m.wav already transcribed\n",
      "File blob_DqrmgyJ.wav already transcribed\n",
      "File blob_mYzWTbN.wav already transcribed\n",
      "File blob_ZZo0EQe.wav already transcribed\n",
      "File blob_uhBrHpE.wav already transcribed\n",
      "File blob_N6nHE2k.wav already transcribed\n",
      "File blob_ZaMCsKi.wav already transcribed\n",
      "File blob_cnMNhzR.wav already transcribed\n",
      "File blob_aMQAsns.wav already transcribed\n",
      "File blob_s0mfMxT.wav already transcribed\n",
      "File blob_TghDmEP.wav already transcribed\n",
      "File blob_imKcAu2.wav already transcribed\n",
      "File blob_TRuxCFT.wav already transcribed\n",
      "File blob_xyOjlVH.wav already transcribed\n",
      "File blob_NNwN0EQ.wav already transcribed\n",
      "File blob_Szysw5i.wav already transcribed\n",
      "File blob_jUZpMvh.wav already transcribed\n",
      "File blob_KEVjp5s.wav already transcribed\n",
      "File blob_tdN7bxu.wav already transcribed\n",
      "File blob_F4eR8SJ.wav already transcribed\n",
      "File blob_FviMkbb.wav already transcribed\n",
      "File blob_R94PBYr.wav already transcribed\n",
      "File blob_LKyMZxD.wav already transcribed\n",
      "File blob_19OS2U5.wav already transcribed\n",
      "File blob_E11VnWI.wav already transcribed\n",
      "File blob_zKyEAmW.wav already transcribed\n",
      "File blob_w8HYSh0.wav already transcribed\n",
      "File blob_P4YhINe.wav already transcribed\n",
      "File blob_TOozGGO.wav already transcribed\n",
      "File blob_ocwADFK.wav already transcribed\n",
      "File blob_4QtpfDi.wav already transcribed\n",
      "File blob_Xu5uJJ2.wav already transcribed\n",
      "File blob_V5IdWfk.wav already transcribed\n",
      "File blob_znsH6Wn.wav already transcribed\n",
      "File blob_ZHP6nNH.wav already transcribed\n",
      "File blob_f1x40yc.wav already transcribed\n",
      "File blob_kZhrZOQ.wav already transcribed\n",
      "File blob_dRV5oKg.wav already transcribed\n",
      "File blob_cipF9FU.wav already transcribed\n",
      "File blob_htWcumt.wav already transcribed\n",
      "File blob_ff1aINK.wav already transcribed\n",
      "File blob_lYqrfTn.wav already transcribed\n",
      "File blob_QJKtqul.wav already transcribed\n",
      "File blob_JifvZJh.wav already transcribed\n",
      "File blob_AAbNtyH.wav already transcribed\n",
      "File blob_6vGkQY8.wav already transcribed\n",
      "File blob_lNNGj9P.wav already transcribed\n",
      "File blob_1CsfM8n.wav already transcribed\n",
      "File blob_uR6AGON.wav already transcribed\n",
      "File blob_KOblwbc.wav already transcribed\n",
      "File blob_VYvAUJw.wav already transcribed\n",
      "File blob_PvK8qdG.wav already transcribed\n",
      "File blob_x14HibK.wav already transcribed\n",
      "File blob_3rzfsmx.wav already transcribed\n",
      "File blob_s1fS4np.wav already transcribed\n",
      "File blob_PTdGU49.wav already transcribed\n",
      "File blob_gS7Duer.wav already transcribed\n",
      "File blob_7asRgBK.wav already transcribed\n",
      "File blob_dZL7P4Z.wav already transcribed\n",
      "File blob_igG5F0r.wav already transcribed\n",
      "File blob_S9Y7JbX.wav already transcribed\n",
      "File blob_FUYF7b8.wav already transcribed\n",
      "File blob_gD23Z79.wav already transcribed\n",
      "File blob_7C8eT3k.wav already transcribed\n",
      "File blob_mnyVQqz.wav already transcribed\n",
      "File blob_Gl8YbpM.wav already transcribed\n",
      "File blob_JFjislD.wav already transcribed\n",
      "File blob_ArkgIMu.wav already transcribed\n",
      "File blob_tIfZZns.wav already transcribed\n",
      "File blob_OU3UBzI.wav already transcribed\n",
      "File blob_RWh7esI.wav already transcribed\n",
      "File blob_lJyelVO.wav already transcribed\n",
      "File blob_Pw359bj.wav already transcribed\n",
      "File blob_oUbRDqh.wav already transcribed\n",
      "File blob_71DRU46.wav already transcribed\n",
      "File blob_6THGegR.wav already transcribed\n",
      "File blob_FcdzXYT.wav already transcribed\n",
      "File blob_IgYQt5Z.wav already transcribed\n",
      "File blob_FrKMjWW.wav already transcribed\n",
      "File blob_7ddLnkV.wav already transcribed\n",
      "File blob_IOMVHa8.wav already transcribed\n",
      "File blob_sECyzPP.wav already transcribed\n",
      "File blob_slb6iz5.wav already transcribed\n",
      "File blob_DQxVBcd.wav already transcribed\n",
      "File blob_2Vmp8iJ.wav already transcribed\n",
      "File blob_LPCZEan.wav already transcribed\n",
      "File blob_Ig0pubk.wav already transcribed\n",
      "File blob_3qVhU86.wav already transcribed\n",
      "File blob_jxf0b6n.wav already transcribed\n",
      "File blob_YoO5hBg.wav already transcribed\n",
      "File blob_UuLDNVa.wav already transcribed\n",
      "File blob_yZDvWm6.wav already transcribed\n",
      "File blob_1bf8seO.wav already transcribed\n",
      "File blob_AizgeKY.wav already transcribed\n",
      "File blob_XiFfrS5.wav already transcribed\n",
      "File blob_gJhR2F7.wav already transcribed\n",
      "File blob_R4fDKpl.wav already transcribed\n",
      "File blob_xLrJUVr.wav already transcribed\n",
      "File blob_SnzkWqe.wav already transcribed\n",
      "File blob_LMW0D8I.wav already transcribed\n",
      "File blob_F3QyS4e.wav already transcribed\n",
      "File blob_U446ase.wav already transcribed\n",
      "File blob_u4ORCDN.wav already transcribed\n",
      "File blob_5Vgn0g9.wav already transcribed\n",
      "File blob_geIxP37.wav already transcribed\n",
      "File blob_zlUV0uS.wav already transcribed\n",
      "File blob_oesOVcm.wav already transcribed\n",
      "File blob_2g7kPhg.wav already transcribed\n",
      "File blob_1YOcXNS.wav already transcribed\n",
      "File blob_XddP7gr.wav already transcribed\n",
      "File blob_QcR0rYv.wav already transcribed\n"
     ]
    }
   ],
   "source": [
    "for answer in full_answers:\n",
    "    for key, value in answer.items():\n",
    "        if not isinstance(value, str) or not value.endswith(\".wav\"):\n",
    "            continue\n",
    "        if value not in files:\n",
    "            print(f\"File {value} not found\")\n",
    "            continue\n",
    "        if value.replace(\".wav\", \".txt\") in os.listdir(local_path):\n",
    "            print(f\"File {value} already transcribed\")\n",
    "            answer[key] = open(\n",
    "                f\"{local_path}/{value.replace('.wav', '.txt')}\", \"r\"\n",
    "            ).read()\n",
    "            continue\n",
    "        answer[key] = transcribe_audio(\n",
    "            f\"{local_path}/{value}\", language=answer[\"language\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed53a2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for answer in full_answers:\n",
    "    for key, value in answer.items():\n",
    "        if not isinstance(value, str) or not value.endswith(\".wav\"):\n",
    "            continue\n",
    "        if value not in files:\n",
    "            print(f\"File {value} not found\")\n",
    "            continue\n",
    "        if value.replace(\".wav\", \".txt\") in os.listdir(local_path):\n",
    "            print(f\"File {value} already transcribed\")\n",
    "            answer[key] = open(\n",
    "                f\"{local_path}/{value.replace('.wav', '.txt')}\", \"r\"\n",
    "            ).read()\n",
    "            continue\n",
    "        answer[key] = transcribe_audio(\n",
    "            f\"{local_path}/{value}\", language=answer[\"language\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f05432",
   "metadata": {},
   "source": [
    "## Scoring the validated scales\n",
    "\n",
    "We are using the HADS and RQ scales to determine the anxiety and depression, and the attachment style status accordingly.\n",
    "\n",
    "### HADS - Hospital Anxiety and Depression Scale\n",
    "A self report tool developed in 1983 by [Zigmond and Snaith](https://pubmed.ncbi.nlm.nih.gov/6880820/). It allows a dimensional scoring of the status of the patient and allows the better understanding of the psychiatric and psychological needs of a hospital patient.\n",
    "\n",
    "The scale is composed of 14 items in a 4-item likert scale pointed from 0 to 3 and dedicates 7 items to depression and anxiety respectively. The scoring occurs by aggregating the values of each item and group, therefore, distributed as follows:\n",
    "- Scores 0 to 7: Normal\n",
    "- Scores 8 to 10: Borderline abnormal (at risk(?))\n",
    "- Scores 11 - 21:  Abnormal (Usually requires a proper diagnosis and treatment)\n",
    "\n",
    "\n",
    "### RQ - The Relationship Questionnaire\n",
    "A self report questionnaire developed in 1991 by [Bartholomew & Horowitz](https://pubmed.ncbi.nlm.nih.gov/1920064/) composed of two segments with the objective of measuring the adult attachment styles. the styles are defined on two axis: Avoidance and anxiety.\n",
    "The avoidance axis refers to the internal model of others and the anxiety to the internal model of self and usually presented as a cartesian plane, in which an adult may have a combination of both axis.\n",
    "The four general cartesian regions in the adult attachment are:\n",
    "**Secure**: Low avoidance - Low anxiety: The person considers others and themselves as trustworthy.\n",
    "**Preoccupied**: Low avoidance - High anxiety: The person considers others are more trustworthy and generate a dependency due to internal low self-worth.\n",
    "**Dismissing**: High avoidance - Low anxiety: The person has low trust on others but is self-reliant (opossite to the one above).\n",
    "**Fearful**: High avoidance - High anxiety: The person does not trust others and has low self worth.\n",
    "\n",
    "In this scale a person will first select a paragraph that reflects directly one of the styles above, then rates the agreement with each style prototype in a 7-item likert scales. From the answers in the likert scales the values on both axis are extracted and calculated as follows:\n",
    "- **Model of self**: (secure + dissmissing) - (preocupied + fearful)\n",
    "- **Model of others**: (secure + preocupied) - (dissmissing + fearful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9dd44b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score HADS and RQ from the full_answers\n",
    "for answer in full_answers:\n",
    "    answer[\"HADS score - Anxiety\"], answer[\"HADS score - Depression\"] = score_HADS({f\"HADS {i}\": answer.get(f\"HADS {i}\") for i in range(1, 15)})\n",
    "    answer[\"RQ score\"] = score_RQ({f\"RQ{i}\": answer.get(f\"RQ{i}\") for i in range(1, 6)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8edb93",
   "metadata": {},
   "source": [
    "## Invoquing OLlama model using the prompts\n",
    "\n",
    "The audio tracks were transcribed and will be fed in a custom prompt to a local Llama2 model using the [OLlama framework](https://ollama.com/)([Git hub documentation](https://github.com/ollama/ollama)).\n",
    "\n",
    "The model has been pretrained by setting the `seed`, to ensure more deterministic results, and with a preparation prompt with extra instructions. The model is asked to act as a psiquiatrist assistant and given some context about the attachment styles, following the best practices mentioned in the literature ([1](https://arxiv.org/abs/2309.09128), [2](https://www.jmir.org/2023/1/e50638/), [3](https://link.springer.com/chapter/10.1007/978-981-99-7962-2_30)) The prompts and settings used in setup are available in the file `Modelfile` in this repository.\n",
    "\n",
    "\n",
    "Then for each participant we agreggate all answers to the images and ask the model to determine the attachment style and two percentages for anxiety and depression afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92f4f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the prompt to be fed to the LLM\n",
    "for answer in full_answers:\n",
    "    answer[\"prompts\"] = []\n",
    "    for key in IMAGE_KEYS:\n",
    "        if answer.get(key) is None:\n",
    "            continue\n",
    "        answer.get(\"prompts\").append(\n",
    "            f\"{key[0:8]} - {QUESTIONS.get(answer.get('language')).get(key[-1])} {answer[key]}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f88a148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful Avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Preoccupied.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Preoccupied attachment style. Likelihood of anxiety: 70%, likelihood of depression: 40%.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n",
      "Found an answer with no extra lines Attachment style: Fearful avoidant.\n"
     ]
    }
   ],
   "source": [
    "# request the LLM analysis\n",
    "\n",
    "for answer in full_answers:\n",
    "    answer[\"llm\"] = []\n",
    "    answer[\"llm_attachment_style\"] = []\n",
    "    answer[\"llm_anxiety\"] = []\n",
    "    answer[\"llm_depression\"] = []\n",
    "    for prompt in answer.get(\"prompts\"):\n",
    "        llm_answer = request_llm_analysis([prompt], language=answer.get(\"language\"))\n",
    "        answer.get(\"llm\").append(llm_answer)\n",
    "        for llm_ans in llm_answer.split(\"\\n\"):\n",
    "            llm_ans_lower = llm_ans.lower()\n",
    "            if \"attachment style\" in llm_ans_lower:\n",
    "                style_name = ''.join(filter(str.isalpha, llm_ans_lower.replace(\"attachment style: \", \"\")))\n",
    "                answer.get(\"llm_attachment_style\").append(style_name)\n",
    "            if len(llm_answer.split(\"\\n\")) == 1:\n",
    "                answer.get(\"llm_anxiety\").append(None)\n",
    "                answer.get(\"llm_depression\").append(None)\n",
    "                print(f\"Found an answer with no extra lines {llm_answer}\")\n",
    "                continue\n",
    "            if \"anxiety\" in llm_ans_lower:\n",
    "                answer.get(\"llm_anxiety\").append(get_only_numbers(llm_ans_lower))\n",
    "            if \"depression\" in llm_ans_lower:\n",
    "                answer.get(\"llm_depression\").append(get_only_numbers(llm_ans_lower))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d3ceee",
   "metadata": {},
   "source": [
    "## Store the resulting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "73471843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the answers in a json lines file\n",
    "now_date = datetime.now().strftime(\"%Y-%m-%d--%H-%M\")\n",
    "with open(f\"raw_answers-{now_date}.json\", \"w\") as answers_file:\n",
    "    answers_file.write(json.dumps(full_answers, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32ec0fe",
   "metadata": {},
   "source": [
    "## Final data preparation\n",
    "\n",
    "In order to compare the results from RQ, HADS and the LLM data we need to transform the data from the LLM:\n",
    "* __Attachment style:__ We want to report a percentage of each style to present from the amount of times one style is mentioned in the list.\n",
    "* __Anxiety and Depression:__ An average value from the reported values may allow to compare to HADS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "91d0d8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for answer in full_answers:\n",
    "    # get the llm_attachment_style, llm_anxiety, llm_depression\n",
    "    llm_attachment_style = answer.get(\"llm_attachment_style\")\n",
    "    llm_anxiety = answer.get(\"llm_anxiety\")\n",
    "    llm_depression = answer.get(\"llm_depression\")\n",
    "    # calculate the average of the llm_anxiety and llm_depression\n",
    "    anxiety_aux = [int(i) for i in llm_anxiety if i is not None]\n",
    "    llm_anxiety_average = sum(anxiety_aux) / len(anxiety_aux) if len(anxiety_aux) > 0 else None\n",
    "    depression_aux = [int(i) for i in llm_depression if i is not None]\n",
    "    llm_depression_average = sum(depression_aux) / len(depression_aux) if len(depression_aux) > 0 else None\n",
    "    answer[\"llm_anxiety_average\"] = llm_anxiety_average\n",
    "    answer[\"llm_depression_average\"] = llm_depression_average\n",
    "    # get the amount of times the attachment style was mentioned\n",
    "    attachment_styles_aux = {}\n",
    "    for style in llm_attachment_style:\n",
    "        if style in attachment_styles_aux:\n",
    "            attachment_styles_aux[style] += 1\n",
    "        else:\n",
    "            attachment_styles_aux[style] = 1\n",
    "    # get the percentage of the attachment styles\n",
    "    total = sum(attachment_styles_aux.values())\n",
    "    attachment_styles = {k: v / total for k, v in attachment_styles_aux.items()}\n",
    "    answer[\"llm_attachment_styles_percentages\"] = attachment_styles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "228e49ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the answers in a json lines file\n",
    "now_date = datetime.now().strftime(\"%Y-%m-%d--%H-%M\")\n",
    "with open(f\"answers-{now_date}.json\", \"w\") as answers_file:\n",
    "    answers_file.write(json.dumps(full_answers, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188fa552",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "\n",
    "We will start doing a simple exploratory analysis of the data, checking the amount of people and the distribution of the demographic data gathered from the surveys.\n",
    "Then we will compare the results got from the HADS and RQ to the results from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d9d6ab59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Demographic - Age</th>\n",
       "      <th>Demographic - Gender</th>\n",
       "      <th>Demographic - Education level</th>\n",
       "      <th>Demographic - Relationship status</th>\n",
       "      <th>Demographic - Native speaker</th>\n",
       "      <th>Demographic - speech diagnosis</th>\n",
       "      <th>Demographic - psychiatric disorder</th>\n",
       "      <th>Demographic - Substances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>23</td>\n",
       "      <td>female</td>\n",
       "      <td>bachelor</td>\n",
       "      <td>single</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Demographic - Age Demographic - Gender Demographic - Education level  \\\n",
       "count                 10                   10                            10   \n",
       "unique                 9                    2                             3   \n",
       "top                   23               female                      bachelor   \n",
       "freq                   2                    5                             6   \n",
       "\n",
       "       Demographic - Relationship status Demographic - Native speaker  \\\n",
       "count                                 10                           10   \n",
       "unique                                 3                            1   \n",
       "top                               single                          yes   \n",
       "freq                                   4                           10   \n",
       "\n",
       "       Demographic - speech diagnosis Demographic - psychiatric disorder  \\\n",
       "count                              10                                 10   \n",
       "unique                              1                                  1   \n",
       "top                                no                                 no   \n",
       "freq                               10                                 10   \n",
       "\n",
       "       Demographic - Substances  \n",
       "count                        10  \n",
       "unique                        1  \n",
       "top                          no  \n",
       "freq                         10  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the information into a Dataframe\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(full_answers)\n",
    "# describe the dataframe to get the statistics\n",
    "# check only for the columns with the 'Demographic - ' prefix\n",
    "demographics = df[\n",
    "    [col for col in df.columns if col.startswith(\"Demographic - \")]\n",
    "]\n",
    "demographics.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "14ac5de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Demographic - Gender\n",
       "female    5\n",
       "male      5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics[\"Demographic - Gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ea924dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('59', '23')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the max and min age\n",
    "demographics[\"Demographic - Age\"].max(), demographics[\"Demographic - Age\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6222d9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   HADS score - Anxiety  HADS score - Depression  llm_depression_average  \\\n",
      "0                     4                        6                      45   \n",
      "1                     7                        7                      53   \n",
      "2                     6                        8                      48   \n",
      "3                     2                        5                      48   \n",
      "4                     4                        7                      44   \n",
      "5                     5                        6                      38   \n",
      "6                     4                        6                      39   \n",
      "7                     1                        9                      43   \n",
      "8                     3                        6                      58   \n",
      "9                     2                        4                      35   \n",
      "\n",
      "   llm_anxiety_average  \n",
      "0                   63  \n",
      "1                   69  \n",
      "2                   65  \n",
      "3                   65  \n",
      "4                   57  \n",
      "5                   55  \n",
      "6                   61  \n",
      "7                   61  \n",
      "8                   75  \n",
      "9                   51  \n"
     ]
    }
   ],
   "source": [
    "# get HADS scores and llm depression and anxiety averages\n",
    "df[\"llm_depression_average\"] = df[\"llm_depression_average\"].apply(\n",
    "    lambda x: int(x) if x is not None else None\n",
    ")\n",
    "df[\"llm_anxiety_average\"] = df[\"llm_anxiety_average\"].apply(\n",
    "    lambda x: int(x) if x is not None else None\n",
    ")\n",
    "\n",
    "# print the columns with the HADS score and the llm depression and anxiety averages as a table\n",
    "print(\n",
    "    df[\n",
    "        [\n",
    "            \"HADS score - Anxiety\",\n",
    "            \"HADS score - Depression\",\n",
    "            \"llm_depression_average\",\n",
    "            \"llm_anxiety_average\",\n",
    "        ]\n",
    "    ]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
